---
title: "mlr3 benchmarking"
author: "Hud Wahab"
output: html_notebook
echo: 'https://github.com/hududed/demo-mbo/blob/main/mlr3-benchmarking.Rmd'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Why benchmark?

[Benchmarking](https://mlr3book.mlr-org.com/performance.html#benchmarking) is used to compare the performance of different models, for example models trained with different learners, on different tasks, or with different resampling methods. This is usually done to obtain an overview of how different methods perform across different tasks. We cover how to

```{r}

library(ggplot2)
library(scales)

library(mlr3)
library(mlr3learners)
library(mlr3extralearners)
library(mlr3viz)
library(mlr3pipelines)
```

We first set the seed to allow reproducibility of the code.

```{r}
set.seed(123)
```

We can set the different learner models:

```{r}
learners = list(lrn("regr.featureless"),
                lrn("regr.lm"),
                lrn("regr.ranger"),
                lrn("regr.km", nugget.stability = 1e-8, covtype = "powexp"),
                lrn("regr.ksvm"),
                lrn("regr.rpart"))
```

Wrap each learner in a one-hot encoder for categorical features (gas)

```{r}
learners = lapply(learners, function(l) po("encode") %>>% po("learner", l))
```

Load the data and remove meta-data columns

```{r}

d.goq = subset(read.csv("GOQ.csv", stringsAsFactors = TRUE), select = -c(campaign, initial))
task.goq = TaskRegr$new(id = "GOQ", backend = d.goq, target = "target")

d.gopi = subset(read.csv("GOPI.csv", stringsAsFactors = TRUE), select = -c(campaign, initial))
task.gopi = TaskRegr$new(id = "GOPI", backend = d.gopi, target = "target")

d.pi = subset(read.csv("PI.csv", stringsAsFactors = TRUE), select = -c(campaign, initial))
task.pi = TaskRegr$new(id = "PI", backend = d.pi, target = "target")
```

Run each learner on each task, with a 3-fold CV (to save time)

```{r}
design = benchmark_grid(
    tasks = list(task.goq, task.gopi, task.pi),
    learners = learners,
    resamplings = rsmp("cv", folds = 3)
)
bmr = benchmark(design)
```

Again, if we want to know what kind of re-sampling is available:

```{r}
as.data.table(mlr_resamplings)
```

The most common use of [mlr3viz](https://mlr3viz.mlr-org.com) is the `autoplot()` function, where the type of the object passed determines the type of the plot. Plotting types can be found by running `?autoplot.X`. For example, the documentation of plots for regression tasks can be found by running `?autoplot.TaskRegr`.

Here, we output the `Benchmark` object easily:

```{r}
autoplot(bmr, measure = msr("regr.mae")) +
    theme(text = element_text(size=15),
          axis.text.x = element_text(angle = 55, hjust = 1)) +
    expand_limits(y = 0)
```

That's it! Now we can compare how the different models perform on our data.

The data `GOQ`, `PI`, `GOPI` used in this notebook was taken from our [recent publication](https://ebooks.iospress.nl/volumearticle/60357); feel free to use these data for any materials science physical experiments benchmarking, but please cite appropriately:

    @incollection{kotthoff2022optimizing,
      title={Optimizing Laser-Induced Graphene Production},
      author={Kotthoff, Lars and Dey, Sourin and Heil, Jake and Jain, Vivek and Muller, Todd and Tyrrell, Alexander and Wahab, Hud and Johnson, Patrick},
      booktitle={PAIS 2022},
      pages={31--44},
      year={2022},
      publisher={IOS Press}
    }
